{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "Probability involves the generation of random variables from known distributions. Statistics deals with estimation of distribution parameters from actual data. With estimated distributions in hand, we proceed to the next step, which is the simulation of random variables for the purpose of risk management.\n",
    "\n",
    "**Monte Carlo (MC)** simulations are central to financial engineering and risk management. They allow financial engineers to price complex financial instruments and risk managers to build the distribution of portfolios that are too complex to model analytically.\n",
    "\n",
    "Simulation results depend heavily on the model's assumptions: the shape of the distribution, the parameters, and the pricing functions.\n",
    "\n",
    "- [Simulations with One Random Variable](#simulation_with_one_random_variable)\n",
    "- [Implement Simulations](#implement_simulations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"simulation_with_one_random_variable\">Simulation with One Random Variable</a>\n",
    "Simulations involve creating artificial random variables with properties similar to those of the risk factors in the portfolio. These include stock prices, exchange rates, bond yields or prices, and commodity prices.\n",
    "### Simulate Markov Processes\n",
    "In efficient markets, financial prices should display a random walk pattern. More precisely, prices are assumed to follow a **Markov process**, which is a particular stochastic process independent of its past history; the entire distribution of the future price relies on the current price only. The past history is irrelevant. These processes are built from the following components, described in order of increasing complexity:\n",
    "- **The Wiener process:** this describes a variable $\\Delta z$, whose change is measured over the interval $\\Delta t$ such that its mean change is $0$ and variance proportional to $\\Delta t$:\n",
    "\n",
    "$\\Delta z \\sim N(0, \\Delta t)$\n",
    "\n",
    "If $\\epsilon$ is a standard normal variable $N(0, 1)$, this can be written as $\\Delta z = \\epsilon \\sqrt{\\Delta t}$. In addition, the increment $\\Delta z$ are independent across time.\n",
    "- **The generalized Wiener process:** this describes a variable $\\Delta x$ built up from a Wiener process, with, in addition, a constant trend $a$ per unit of time and volatility $b$:\n",
    "\n",
    "$\\Delta x = a \\Delta t + b \\Delta z$\n",
    "\n",
    "A particular case is the **martingale**, which is a zero-drift stochastic process, $a = 0$, which leads to $E(\\Delta x) = 0$. This has the convenient property that the expectation of a future value is the current value\n",
    "\n",
    "$E(x_{T}) = x_{0}$\n",
    "\n",
    "- **The Ito process:** this describes a generalized Wienee process, whose trend and volatility depend on the current value of the underlying variable and time:\n",
    "\n",
    "$\\Delta x = a(x, t)\\Delta t + b(x, t)\\Delta z$\n",
    "\n",
    "This is a Markov process because the distribution depends only on the current value of the random variable $x$, as well as time. In addition, the innovation in this process has a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The geometric Brownian motion (GBM)\n",
    "A particular example of Ito process is the **geometric Brownian motion (GBM)**, which is described for the variable $S$ as \n",
    "\n",
    "$\\Delta S = \\mu S \\Delta t + \\sigma S \\Delta z$\n",
    "\n",
    "The process is geometric because the trend and volatility terms are proportional to the current value of $S$. This is typically the case for stock prices, for which rates of return appear to be more stationary than raw dollar returns, $\\Delta S$. It is also used for currencies. Because $\\Delta S/S$ represents the capital appreciation only, abstracting from dividend payments, $\\mu$ represents the expected total rate of return on the asset minus the rate of income payment, or dividend yield in the case of stocks.\n",
    "\n",
    "This model is particularly important because it is the underlying process for the Black-Scholes formula. The key feature of this distribution is the fact that the volatility is proportional to $S$. This ensures that the stock price will stay positive. Indeed, as the stock price falls, its variance decreases, which makes it unlikely to experience a large down move that would push the price into negative values. As the limit if this model is a normal distribution for $dS/S = dln(S)$, $S$ follows a **lognormal distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process implies that, over an interval $T - t = \\tau$, the logarithm of the ending price is distributed as\n",
    "\n",
    "$ln(S_{T}) = ln(S_{t}) + (\\mu - \\sigma^{2}/2)\\tau + \\sigma\\sqrt{\\tau}\\epsilon$\n",
    "\n",
    "where $\\epsilon$ is a standardized normal variable.\n",
    "\n",
    "Whether a lognormal distribution is much better than the normal distribution depends on the horizon considered. If the horizon is one day only, the choice of the lognormal versus normal assumption does not really matter. It is highly unlikely that the stock price would drop below $0$ in one day, given typical volatilities. However, if the horizon is measured in years, the two assumptions do lead to different results. The lognormal distribution is more realistic as it prevents prices from turning negative.\n",
    "\n",
    "In simulations, this process is approximated by small steps with a normal distribution with mean and variance given by\n",
    "\n",
    "$\\frac{\\Delta S}{S} \\sim N(\\mu\\Delta t, \\sigma^{2}\\Delta t)$\n",
    "\n",
    "To simulate the future price path for $S$, we start from the current price $S_{t}$ and generate a sequence of independent standard normal variables $\\epsilon$, for $i = 1, 2,..., n$. The next price, $S_{t+1}$, is built as $S_{t+1} = S_{t} + S_{t}(\\mu\\Delta t + \\sigma\\epsilon_{1}\\sqrt{\\Delta t})$. The following price, $S_{t+2}$, is taken as $S_{t+1} + S_{t+1}(\\mu\\Delta t+\\sigma\\epsilon_{2}\\sqrt{\\Delta t})$, and so on until we reach the target horizon, at which point the price $S_{t+n} = S_{T}$ should have a distribution close to the lognormal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $K$ as the number of **replications**, or random trials. Each leads to a simulated final value $S^{k}_{T}$. This generates a distribution of simulated prices $S_{T}$. With just one step $n = 1$, the distribution must be normal. As the number of steps $n$ grows large, the distribution tends to a lognormal distribution.\n",
    "\n",
    "While very useful for modeling stock prices, this model has shortcomings. Price increments are assumed to have a normal distribution. In practice, we observe that price changes for most financial assets typically have fatter tails than the normal distribution. Returns may also experience changing variances.\n",
    "\n",
    "In addition, as the time interval $\\Delta t$ shrinks, the volatility shrinks as well. This implies that large discontinuities cannot occur over short intervals. In reality, some assets experience discrete jumps, such as commodities, or securities issued by firms that go bankrupt. In such cases, the stochastic process should be changed to accommodate these observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw random variables\n",
    "Most spreadsheets or statistical packages have functions that can generate uniform or standard normal random variables. This can be easily extended to distributions that better reflect the data (e.g., with fatter tails or nonzero skewness).\n",
    "\n",
    "The methodology involves the inverse cumulative probability distribution function ($p.d.f.$). Take the normal distribution as an example. By definition, the cumulative $p.d.f.$ $N(x)$ is always between $0$ and $1$. Because we have an analytical formula for this function, it can be easily inverted.\n",
    "\n",
    "First, we generate a uniform random variable $u$ drawn from $U(0, 1)$. Next, we compute $x$ such that $u = N(x)$, or $x = N^{-1}(u)$. The variable can be transformed into any normal variable by multiplying by the standard deviation and adding the mean. More generally, any distribution function can be generated as long as the cumulative distribution function can be inverted.\n",
    "### Simulate yields\n",
    "The $GBM$ process is widely used for stock prices and currencies. Fixed-income products are another matter, however.\n",
    "\n",
    "Bond prices display long-term reversion to the face value, which represents the repayment of principal at maturity (assuming there is no default). Such a process is inconsistent with $GBM$ process, which displays no such mean reversion. The volatility of bond prices also changes in a predictable fashion, as duration shrinks to $0$. Similarly, commodities often display mean reversion.\n",
    "\n",
    "These features can be taken into account by modeling bond yields directly in a first step. In the next step, bond prices are constructed from the value of yields and a pricing function. The dynamics of interest rates $r_{t}$ can be modeled by \n",
    "\n",
    "$\\Delta{r_{t}} = \\kappa(\\theta - r_{t})\\Delta t + \\sigma r_{t}^{\\gamma}\\Delta z_{t}$\n",
    "\n",
    "where $\\Delta Z_{t}$ is the usual Wiener process. Here, we assume that $0 \\leq \\kappa < 1, \\theta \\geq 0, \\sigma \\geq 0$. Because there is only one stochastic variable for yields, the model is called a **one-factor model**.\n",
    "\n",
    "This Markov process has a number of interesting features. First, it displays mean reversion to a long-run value of $\\theta$. The parameter $\\kappa$ governs the speed of mean reversion. When the current interest rate is high (i.e., $r_{t} > \\theta$), the model creates a negative drift $\\kappa(\\theta - r_{t})$ toward $\\theta$. Conversely, low current rates create a positive drift toward $\\theta$.\n",
    "\n",
    "The second feature is the volatility process. This model includes the **Vasicek model** when $\\gamma = 0$. Changes in yields are normally distributed because $\\Delta r$ is then a linear function of $\\Delta z$, which is itself normal. The Vasicek model is particular convenient because it leads to closed-form solutions for many fixed-income products. The problem, however, is that it could potentially lead to negative interest rates when the initial rate starts from a low value. This is because the volatility of the change in rates does not depend on the level, unlike that in the geometric Brownian motion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\Delta r_{t}$ equation is more general, however, because it includes a power of the yield in the variance function. With $\\gamma = 1$, this is the **lognormal model**. Ignoring the trend, this gives $\\Delta r_{t} = \\sigma r_{t}\\Delta z_{t}$, or $\\Delta r_{t}/r_{t} = \\sigma \\Delta z_{t}$. This implies that the rate of change in the yield $dr/r$ has a fixed variance. Thus, as with the $GBM$ model, smaller yields lead to smaller movements, which makes it unlikely the yield will drop below $0$. This model is more appropriate than the normal model when the initial yield is close to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $gamma = 0.5$, this is the **Cox, Ingersoll, and Ross (CIR) model**. Ultimately, the choice of the exponent $\\gamma$ is an empirical issue. Recent research has shown that $\\gamma = 0.5$ provides a good fit to the data.\n",
    "\n",
    "This class of model is known as **equilibrium models**. They start with some assumptions about economic variables and imply a process for the short-term interest rate $\\gamma$. These models generate a predicted term structure, whose shape depends on the model parameters and the initial short rate. The problem with these model, however, is that they are not flexible enough to provide a good fit to today's term structure. This can be viewed as unsatisfactory, especially by practitioners who argue they cannot rely on a model that cannot be trusted to price today's bonds.\n",
    "\n",
    "In contrast, **no-arbitrage models** are designed to be consistent with today's term structure. In this class of models, the term structure is an input into the parameter estimation. The earliest model of this type was the **Ho and Lee model**:\n",
    "\n",
    "$r_{t} = \\theta(t)\\Delta t + \\sigma \\Delta z_{t}$\n",
    "\n",
    "where $theta(t)$ is a function of time chosen so that the model fits the initial term structure. This was extended to incorporate mean reversion in the **Hull and White model**:\n",
    "\n",
    "$\\Delta r_{t} = [\\theta(t) - ar_{t}]\\Delta t + \\sigma \\Delta z_{t}$\n",
    "\n",
    "Finally, the **Heath, Jarrow, and Morton model** goes one step further and assumes that the volatility is a function of time.\n",
    "\n",
    "The downside of these no-arbitrage models is that they do not impose any consistency between parameters estimated over different dates. The function $\\theta(t)$ could be totally different from one day to the next, which is illogical. No-arbitrage models are also more sensitive to outliers, or data errors in bond prices used to fit the term structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial trees\n",
    "Simulations are very useful to mimic the uncertainty in risk factors, especially with numerous risk factors. In some situations, however, it is also useful to describe the uncertainty in prices with discrete trees. When the price can take one of two steps, the tree is said to be **binomial**.\n",
    "\n",
    "The binomial model can be viewed as a discrete equivalent to the geometric Brownian motion. As before, we subdivide the horizon $T$ into $n$ intervals $\\Delta t = T/n$. At each node, the price is assumed to go either up with probability $p$ or down with probability $1 - p$.\n",
    "\n",
    "The parameters $u, d, p$ are chosen so that, for a small time interval, the expected return and variance equal those of the continuous process. One could choose \n",
    "\n",
    "$u = e^{\\sigma\\sqrt{\\Delta t}}, d = (1/u), p = \\frac{e^{\\mu \\Delta t - d}}{u - d}$\n",
    "\n",
    "This matches the mean, for example,\n",
    "\n",
    "$E[\\frac{S_{1}}{S_{0}}] = pu + (1 - p)d = \\frac{e^{\\mu\\Delta t} - d}{u - d}u + \\frac{u - e^{\\mu\\Delta t}}{u - d}d = \\frac{e^{\\mu\\Delta t}(u - d) - du + ud}{u - d} = e^{\\mu \\Delta t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"implement_simulations\">Implement simulations</a>\n",
    "### Simulation for VAR\n",
    "Implementing Monte Carlo (MC) methods for risk management follows these steps:\n",
    "1. Choose a stochastic process for the risk factor price S (i.e., its distribution and parameters, starting from the current value $S_{t}$).\n",
    "2. Generate pseudo-random variables representing the risk factor at the target horizon, $S_{T}$.\n",
    "3. Calculate the value of the portfolio at the horizon, $F_{T}(S_{T})$.\n",
    "4. Repeat steps 2 and 3 as many times as necessary. Call $K$ the number of replications.\n",
    "\n",
    "These steps create a distribution of values, $F^{1}_{T},...,F^{K}_{T}$, which can be sorted to derive the $VAR$. We measure the $c$th quantile $Q(F_{T}, c)$ and the average value $Ave(F_{T})$. If $VAR$ is defined as the deviation from the expected value on the target date, we have\n",
    "\n",
    "$VAR(c) = Ave(F_{t}) - Q(F_{T}, c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation for derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
